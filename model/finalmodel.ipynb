{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, BatchNormalization, Activation, Input, Add, Concatenate,\\\n",
    "                         Bidirectional, SimpleRNN, LSTM, GRU\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Activation, Input, \\\n",
    "    Conv1D, MaxPool1D, Flatten, Concatenate, Add, MaxPooling1D,LSTM\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, \n",
    "    \n",
    "type|: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n",
    "    \n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Quoraquestions.csv\")\n",
    "#answer_count_list = non_anonymous_answer_count+anonymous_answer_count\n",
    "labels = []\n",
    "\n",
    "for i in range(df.shape[0]):   \n",
    "    if(df[\"anonymous_answer_count\"][i] > 0):\n",
    "        labels.append(1)\n",
    "        \n",
    "    else:\n",
    "        labels.append(0)\n",
    "Counter(labels)\n",
    "#Construct the labels of questions: 1 means receiving anonymous answers, 0 means not.\n",
    "xtrain = df[\"question_title_list\"]\n",
    "x_train = np.array(xtrain)\n",
    "y_train = np.array(labels)\n",
    "#x_train contains question texts, y_train contains labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 10028, 1: 1099})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the question texts\n",
    "\n",
    "train_texts = x_train\n",
    "#print(train_texts[0])\n",
    "train_tokens = [tokenize(text) for text in train_texts]\n",
    "#print(train_tokens[0])\n",
    "tokens = []\n",
    "for text in train_tokens:\n",
    "    text2 = [i.lower() for i in text]\n",
    "    text1 = []\n",
    "    for i in text2:\n",
    "        if(i != \"?\"):\n",
    "            text1.append(i)\n",
    "    tokens.append(text1)\n",
    "train_tokens = tokens\n",
    "#print(train_tokens[0])\n",
    "filtered_sentence = [filter_stopwords(text) for text in train_tokens]\n",
    "#print(filtered_sentence[0])\n",
    "#print(len(train_tokens))\n",
    "train_stemmed = [stem(tokens) for tokens in filtered_sentence]\n",
    "#print(train_stemmed[0])\n",
    "#print(len(train_stemmed))\n",
    "train_2_gram = [n_gram(tokens, 2) for tokens in train_stemmed]\n",
    "#print(train_2_gram[0])\n",
    "train_3_gram = [n_gram(tokens, 3) for tokens in train_stemmed]\n",
    "#print(train_3_gram[0])\n",
    "train_feats = list()\n",
    "for i in range(len(train_texts)):\n",
    "    train_feats.append(\n",
    "        train_stemmed[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feats_dict(feats, min_freq=-1, max_freq=-1, max_size=-1):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list(list)\n",
    "    :param min_freq: the lowest fequency that the fequency of a feature smaller than it will be filtered out\n",
    "    :param max_freq: the highest fequency that the fequency of a feature larger than it will be filtered out\n",
    "    return a feature dict that maps features to indices, sorted by frequencies\n",
    "    # Counter document: https://docs.python.org/3.6/library/collections.html#collections.Counter\n",
    "    \"\"\"\n",
    "    # count all features\n",
    "    feat_cnt = Counter(feats) # [\"text\", \"text\", \"mine\"] --> {\"text\": 2, \"mine\": 1}\n",
    "    if max_size > 0 and min_freq == -1 and max_freq == -1:\n",
    "        valid_feats = [f for f, cnt in feat_cnt.most_common(max_size)]\n",
    "    else:\n",
    "        valid_feats = list()\n",
    "        for f, cnt in feat_cnt.most_common():\n",
    "            if (min_freq == -1 or cnt >= min_freq) and \\\n",
    "                (max_freq == -1 or cnt <= max_freq):\n",
    "                valid_feats.append(f)\n",
    "    if max_size > 0 and len(valid_feats) > max_size:\n",
    "        valid_feats = valid_feats[:max_size]        \n",
    "    print(\"Size of features:\", len(valid_feats))\n",
    "    \n",
    "    # build a mapping from features to indices\n",
    "    feats_dict = dict(zip(valid_feats, range(len(valid_feats))))\n",
    "    return feats_dict\n",
    "\n",
    "def get_idf_dict(feats_list):\n",
    "    \"\"\"\n",
    "    :param feats_list: a list of lists of features, type: list(list)\n",
    "    return an idf vector,\n",
    "    \"\"\"\n",
    "    N = len(feats_list)\n",
    "    df_dict = Counter()\n",
    "    for feats in feats_list:\n",
    "        df_dict.update(set(feats))\n",
    "    # IDF: log(1 + N/n)\n",
    "    idf_dict = {f: math.log2(1+N/cnt) for f, cnt in df_dict.items()}\n",
    "    \n",
    "    return idf_dict\n",
    "\n",
    "def get_tfidf_vector(feats, feats_dict, idf_dict):\n",
    "    \"\"\"\n",
    "    :param feats: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    :param idf_dict: a dict from features to idf, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    # TF: 1 + log(f)\n",
    "    tf_dict = {f: 1+math.log2(cnt) for f, cnt in Counter(feats).items()}\n",
    "    for f in tf_dict:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            tf = tf_dict[f]\n",
    "            idf = idf_dict[f]\n",
    "            # set the corresponding element as tf*idf\n",
    "            vector[f_idx] = tf*idf\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of features: 10703\n"
     ]
    }
   ],
   "source": [
    "feats_dict = get_feats_dict(\n",
    "    chain.from_iterable(train_feats))\n",
    "\n",
    "idf_dict = get_idf_dict(train_feats)\n",
    "train_tfidf_feats_matrix = np.vstack([\n",
    "    get_tfidf_vector(f, feats_dict, idf_dict) for f in train_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056\n",
      "[11518 18115 19550 ... 16468 12261   299]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "#Using smote to solve the unbalanced data problems\n",
    "X_resampled_smote, y_resampled_smote = SMOTE().fit_sample(train_tfidf_feats_matrix,y_train)\n",
    "print(len(y_resampled_smote))\n",
    "Counter(y_resampled_smote)\n",
    "x_train = X_resampled_smote\n",
    "y_train = y_resampled_smote\n",
    "data_num= x_train.shape[0]\n",
    "index = np.arange(data_num)  # 生成下标  \n",
    "np.random.shuffle(index)\n",
    "print(index)\n",
    "x_train = x_train[index]\n",
    "y_train = y_train[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def computeAUC(y_true,y_score):\n",
    "    auc = roc_auc_score(y_true,y_score)\n",
    "    print(\"auc = \",auc)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/anaconda3/lib/python3.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train,y_train, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc =  0.9972859963457588\n",
      "auc =  0.9253589548361739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9253589548361739"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using logistic regression to evaluate the classification model.\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "predicted_probs_train = lr.predict_proba(x_train)\n",
    "predicted_probs_train = [x[1] for  x in predicted_probs_train]\n",
    "computeAUC(y_train, predicted_probs_train)\n",
    "\n",
    "predicted_probs_test_new = lr.predict_proba(x_test)\n",
    "predicted_probs_test_new = [x[1] for x in predicted_probs_test_new]\n",
    "computeAUC(y_test, predicted_probs_test_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc =  0.9356448463312719\n",
      "auc =  0.9377458901292293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9377458901292293"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the RandomForest Model to evaluate the classification model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100,\n",
    "                                oob_score= True,\n",
    "                                min_samples_split=2,\n",
    "                                min_samples_leaf=50,\n",
    "                                n_jobs=-1,\n",
    "                                class_weight='balanced_subsample',\n",
    "                                bootstrap=True)\n",
    "\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "predicted_probs_train = rf.predict_proba(x_train)\n",
    "predicted_probs_train = [x[1] for x in predicted_probs_train]\n",
    "computeAUC(y_train, predicted_probs_train)\n",
    "#使用训练的模型来预测test_new数据（validataion data）\n",
    "predicted_probs_test_new = rf.predict_proba(x_test)\n",
    "predicted_probs_test_new = [x[1] for x in predicted_probs_test_new]\n",
    "computeAUC(y_test, predicted_probs_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(input_size, output_size, learning_rate=0.1,\n",
    "                     l2_reg=0.0,\n",
    "                     loss=\"binary_crossentropy\",\n",
    "                     optimizer=\"SGD\",\n",
    "                     metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    :param input_size: the dimension of the input, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param learning_rate: the learning rate for the optimizer\n",
    "    :param l2_reg: the weight for the L2 regularizer\n",
    "    :param loss: the training loss\n",
    "    :param optimizer: the optimizer\n",
    "    :param metric: the metric\n",
    "    return a 1-layer perceptron,\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # the projection layer\n",
    "    model.add(Dense(output_size,\n",
    "                    activation=\"softmax\",\n",
    "                    input_dim=input_size,\n",
    "                    kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                    bias_initializer='zeros',\n",
    "                    kernel_regularizer=keras.regularizers.l2(l2_reg)))\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = max(y_train)\n",
    "# convert each label to a one-hot vector, and then stack vectors as a matrix\n",
    "train_label_matrix = keras.utils.to_categorical(y_train-1, num_classes=num_classes)\n",
    "test_label_matrix = keras.utils.to_categorical(y_test-1, num_classes=num_classes)\n",
    "model = build_classifier(len(feats_dict), num_classes,loss=\"binary_crossentropy\")\n",
    "\n",
    "# train the model\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "history = model.fit(x_train, train_label_matrix,\n",
    "    epochs=5, batch_size=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15042/15042 [==============================] - 3s 190us/step\n",
      "5014/5014 [==============================] - 1s 214us/step\n",
      "training loss: 0.0 training accuracy 1.0\n",
      "test loss: 0.0 test accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "train_score = model.evaluate(x_train, train_label_matrix,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(x_test, test_label_matrix,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "#Train the model using the training sets\n",
    "clf.fit(x_train, y_train)\n",
    "print(clf.score(x_train, y_train)) # 精度\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(C=0.9, kernel='rbf', gamma=20, decision_function_shape='ovo')\n",
    "clf.fit(x_train, y_train.ravel())\n",
    "print(clf.score(x_train, y_train)) # 精度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = clf.predict(x_train)\n",
    "show_accuracy(y_hat, y_train, '训练集')\n",
    "print(clf.score(x_test, y_test))\n",
    "y_hat = clf.predict(x_test)\n",
    "show_accuracy(y_hat, y_test, '测试集')\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_train, y_hat, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost Model\n",
    "model = XGBClassifier(learning_rate=0.1,\n",
    "                      n_estimators=1000,         # 树的个数--1000棵树建立xgboost\n",
    "                      max_depth=6,               # 树的深度\n",
    "                      min_child_weight = 1,      # 叶子节点最小权重\n",
    "                      gamma=0.,                  # 惩罚项中叶子结点个数前的参数\n",
    "                      subsample=0.8,             # 随机选择80%样本建立决策树\n",
    "                      colsample_btree=0.8,       # 随机选择80%特征建立决策树\n",
    "                      objective='multi:softmax', # 指定损失函数\n",
    "                      scale_pos_weight=1,        # 解决样本个数不平衡的问题\n",
    "                      random_state=27            # 随机数\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          eval_set = [(x_test,y_test)],\n",
    "          eval_metric = \"mlogloss\",\n",
    "          early_stopping_rounds = 10,\n",
    "          verbose = True)\n",
    "\n",
    "### make prediction for test data\n",
    "y_pred = model.predict(x_test)\n",
    "computeAUC(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN model\n",
    "def build_Res_Net(input_size, output_size, num_layers, hidden_size,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              layer_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"binary_crossentropy\",\n",
    "              optimizer=\"SGD\",\n",
    "              learning_rate=0.1,\n",
    "              metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    :param input_size: the dimension of the input, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param num_layers: the number of layers, type: int\n",
    "    :param hidden_size: the dimension of the hidden states, type: int\n",
    "    :param activation: the activation type, type: str\n",
    "    :param dropout_rate: the probability of dropout, type: float\n",
    "    :param batch_norm: whether to enable batch normalization, type: bool\n",
    "    :param layer_norm: whether to enable layer normalization, type: bool\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a multi-layer network with residual connections,\n",
    "    # activation\n",
    "    # dropout document: https://keras.io/layers/core/#dropout\n",
    "    # batch normalization document: https://keras.io/layers/normalization/\n",
    "    # layer normalization: https://github.com/CyberZHG/keras-layer-normalization\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_size,))\n",
    "    \n",
    "    if num_layers == 1:\n",
    "        y = Dense(output_size,\n",
    "                  activation=\"softmax\",\n",
    "                  input_dim=input_size,\n",
    "                  kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                  bias_initializer=\"zeros\",\n",
    "                  kernel_regularizer=keras.regularizers.l2(l2_reg))(x)\n",
    "    else:\n",
    "        h = x\n",
    "        for i in range(num_layers-1):\n",
    "            if i == 0:\n",
    "                # fitst layer: input -> hidden\n",
    "                new_h = Dense(hidden_size,\n",
    "                          input_dim=input_size,\n",
    "                          kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                          bias_initializer=\"zeros\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "            else:\n",
    "                new_h = Dense(hidden_size,\n",
    "                          input_dim=hidden_size,\n",
    "                          kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                          bias_initializer=\"zeros\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "            # add layer_norm\n",
    "            if layer_norm:\n",
    "                new_h = LayerNormalization()(new_h)\n",
    "            # add batch_norm\n",
    "            if batch_norm:\n",
    "                new_h = BatchNormalization()(new_h)\n",
    "            # residual connection\n",
    "            if i == 0:\n",
    "                h = new_h\n",
    "            else:\n",
    "                h = Add()([h, new_h])\n",
    "            # add activation\n",
    "            h = Activation(activation)(h)\n",
    "            # add dropout here (set seed as 0 in order to reproduce)\n",
    "            if dropout_rate > 0.0:\n",
    "                h = Dropout(dropout_rate, seed=0)(h)\n",
    "        # last layer: hidden -> class\n",
    "        y = Dense(output_size,\n",
    "                  activation=\"softmax\",\n",
    "                  input_dim=hidden_size,\n",
    "                  kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                  bias_initializer=\"zeros\")(h)\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model = Model(x, y)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15042/15042 [==============================] - 4s 240us/step\n",
      "5014/5014 [==============================] - 2s 405us/step\n",
      "training loss: 1.5248525142669678 training accuracy 1.0\n",
      "test loss: 1.5248525142669678 test accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "num_classes = max(y_train)\n",
    "model = build_Res_Net(input_size=len(feats_dict), output_size=num_classes,\n",
    "                  num_layers=3, hidden_size=100, activation=\"relu\",\n",
    "                  l2_reg=0.005, dropout_rate=0.1)\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "res_history = model.fit(x_train,train_label_matrix,\n",
    "                        validation_split=0.1,\n",
    "                        epochs=5, batch_size=100, verbose=0,\n",
    "                        callbacks=[checkpointer])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(x_train, train_label_matrix,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(x_test, test_label_matrix,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
