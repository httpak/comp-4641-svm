{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, BatchNormalization, Activation, Input, Add, Concatenate,\\\n",
    "                         Bidirectional, SimpleRNN, LSTM, GRU\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Activation, Input, \\\n",
    "    Conv1D, MaxPool1D, Flatten, Concatenate, Add, MaxPooling1D,LSTM\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, \n",
    "    \n",
    "type|: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n",
    "    \n",
    "def filter_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of filtered tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     if token not in stopwords and not token.isnumeric():\n",
    "    #         results.append(token)\n",
    "    # return results\n",
    "\n",
    "    return [token for token in tokens if token not in stopwords and not token.isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Quoraquestions.csv\")\n",
    "#answer_count_list = non_anonymous_answer_count+anonymous_answer_count\n",
    "labels = []\n",
    "\n",
    "for i in range(df.shape[0]):   \n",
    "    if(df[\"anonymous_answer_count\"][i] > 0):\n",
    "        labels.append(1)\n",
    "        \n",
    "    else:\n",
    "        labels.append(0)\n",
    "Counter(labels)\n",
    "#Construct the labels of questions: 1 means receiving anonymous answers, 0 means not.\n",
    "xtrain = df[\"question_title_list\"]\n",
    "x_train = np.array(xtrain)\n",
    "y_train = np.array(labels)\n",
    "#x_train contains question texts, y_train contains labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 10028, 1: 1099})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the question texts\n",
    "\n",
    "train_texts = x_train\n",
    "#print(train_texts[0])\n",
    "train_tokens = [tokenize(text) for text in train_texts]\n",
    "#print(train_tokens[0])\n",
    "tokens = []\n",
    "for text in train_tokens:\n",
    "    text2 = [i.lower() for i in text]\n",
    "    text1 = []\n",
    "    for i in text2:\n",
    "        if(i != \"?\"):\n",
    "            text1.append(i)\n",
    "    tokens.append(text1)\n",
    "train_tokens = tokens\n",
    "#print(train_tokens[0])\n",
    "filtered_sentence = [filter_stopwords(text) for text in train_tokens]\n",
    "#print(filtered_sentence[0])\n",
    "#print(len(train_tokens))\n",
    "train_stemmed = [stem(tokens) for tokens in filtered_sentence]\n",
    "#print(train_stemmed[0])\n",
    "#print(len(train_stemmed))\n",
    "train_2_gram = [n_gram(tokens, 2) for tokens in train_stemmed]\n",
    "#print(train_2_gram[0])\n",
    "train_3_gram = [n_gram(tokens, 3) for tokens in train_stemmed]\n",
    "#print(train_3_gram[0])\n",
    "train_feats = list()\n",
    "for i in range(len(train_texts)):\n",
    "    train_feats.append(\n",
    "        train_stemmed[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feats_dict(feats, min_freq=-1, max_freq=-1, max_size=-1):\n",
    "    \"\"\"\n",
    "    :param data: a list of features, type: list(list)\n",
    "    :param min_freq: the lowest fequency that the fequency of a feature smaller than it will be filtered out\n",
    "    :param max_freq: the highest fequency that the fequency of a feature larger than it will be filtered out\n",
    "    return a feature dict that maps features to indices, sorted by frequencies\n",
    "    # Counter document: https://docs.python.org/3.6/library/collections.html#collections.Counter\n",
    "    \"\"\"\n",
    "    # count all features\n",
    "    feat_cnt = Counter(feats) # [\"text\", \"text\", \"mine\"] --> {\"text\": 2, \"mine\": 1}\n",
    "    if max_size > 0 and min_freq == -1 and max_freq == -1:\n",
    "        valid_feats = [f for f, cnt in feat_cnt.most_common(max_size)]\n",
    "    else:\n",
    "        valid_feats = list()\n",
    "        for f, cnt in feat_cnt.most_common():\n",
    "            if (min_freq == -1 or cnt >= min_freq) and \\\n",
    "                (max_freq == -1 or cnt <= max_freq):\n",
    "                valid_feats.append(f)\n",
    "    if max_size > 0 and len(valid_feats) > max_size:\n",
    "        valid_feats = valid_feats[:max_size]        \n",
    "    print(\"Size of features:\", len(valid_feats))\n",
    "    \n",
    "    # build a mapping from features to indices\n",
    "    feats_dict = dict(zip(valid_feats, range(len(valid_feats))))\n",
    "    return feats_dict\n",
    "\n",
    "def get_idf_dict(feats_list):\n",
    "    \"\"\"\n",
    "    :param feats_list: a list of lists of features, type: list(list)\n",
    "    return an idf vector,\n",
    "    \"\"\"\n",
    "    N = len(feats_list)\n",
    "    df_dict = Counter()\n",
    "    for feats in feats_list:\n",
    "        df_dict.update(set(feats))\n",
    "    # IDF: log(1 + N/n)\n",
    "    idf_dict = {f: math.log2(1+N/cnt) for f, cnt in df_dict.items()}\n",
    "    \n",
    "    return idf_dict\n",
    "\n",
    "def get_tfidf_vector(feats, feats_dict, idf_dict):\n",
    "    \"\"\"\n",
    "    :param feats: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    :param idf_dict: a dict from features to idf, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    # TF: 1 + log(f)\n",
    "    tf_dict = {f: 1+math.log2(cnt) for f, cnt in Counter(feats).items()}\n",
    "    for f in tf_dict:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            tf = tf_dict[f]\n",
    "            idf = idf_dict[f]\n",
    "            # set the corresponding element as tf*idf\n",
    "            vector[f_idx] = tf*idf\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of features: 10703\n"
     ]
    }
   ],
   "source": [
    "feats_dict = get_feats_dict(\n",
    "    chain.from_iterable(train_feats))\n",
    "\n",
    "idf_dict = get_idf_dict(train_feats)\n",
    "train_tfidf_feats_matrix = np.vstack([\n",
    "    get_tfidf_vector(f, feats_dict, idf_dict) for f in train_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056\n",
      "[18826 16377 16172 ...   854  3766 12553]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "#Using smote to solve the unbalanced data problems\n",
    "X_resampled_smote, y_resampled_smote = SMOTE().fit_sample(train_tfidf_feats_matrix,y_train)\n",
    "print(len(y_resampled_smote))\n",
    "Counter(y_resampled_smote)\n",
    "x_train = X_resampled_smote\n",
    "y_train = y_resampled_smote\n",
    "data_num= x_train.shape[0]\n",
    "index = np.arange(data_num)  # 生成下标  \n",
    "np.random.shuffle(index)\n",
    "print(index)\n",
    "x_train = x_train[index]\n",
    "y_train = y_train[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def computeAUC(y_true,y_score):\n",
    "    auc = roc_auc_score(y_true,y_score)\n",
    "    print(\"auc = \",auc)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/anaconda3/lib/python3.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train,y_train, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc =  0.9971840163841104\n",
      "auc =  0.92137218954498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.92137218954498"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using logistic regression to evaluate the classification model.\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "predicted_probs_train = lr.predict_proba(x_train)\n",
    "predicted_probs_train = [x[1] for  x in predicted_probs_train]\n",
    "computeAUC(y_train, predicted_probs_train)\n",
    "\n",
    "predicted_probs_test_new = lr.predict_proba(x_test)\n",
    "predicted_probs_test_new = [x[1] for x in predicted_probs_test_new]\n",
    "computeAUC(y_test, predicted_probs_test_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc =  0.9341712262158213\n",
      "auc =  0.9341908722277392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9341908722277392"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the RandomForest Model to evaluate the classification model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100,\n",
    "                                oob_score= True,\n",
    "                                min_samples_split=2,\n",
    "                                min_samples_leaf=50,\n",
    "                                n_jobs=-1,\n",
    "                                class_weight='balanced_subsample',\n",
    "                                bootstrap=True)\n",
    "\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "predicted_probs_train = rf.predict_proba(x_train)\n",
    "predicted_probs_train = [x[1] for x in predicted_probs_train]\n",
    "computeAUC(y_train, predicted_probs_train)\n",
    "#使用训练的模型来预测test_new数据（validataion data）\n",
    "predicted_probs_test_new = rf.predict_proba(x_test)\n",
    "predicted_probs_test_new = [x[1] for x in predicted_probs_test_new]\n",
    "computeAUC(y_test, predicted_probs_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
